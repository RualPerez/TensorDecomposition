{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda is available:  True \n",
      "Use cuda:  True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import torch.utils as tutils\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from sklearn.metrics import accuracy_score # used in training\n",
    "\n",
    "import math\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "# Use GPU\n",
    "use_cuda = torch.cuda.is_available()\n",
    "#use_cuda = False # Set to false, if the code should be run on the cpu even if cuda is available\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "print(\"Cuda is available: \",torch.cuda.is_available(),\"\\nUse cuda: \",use_cuda)\n",
    "\n",
    "# PCA of the training set\n",
    "perform_pca = True\n",
    "n_PCA = 5 # number of PCA components\n",
    "\n",
    "# Specify the size of the training, cross validation and test set\n",
    "train_samples = 1024 #12800\n",
    "cv_samples    = 512 #1024\n",
    "test_samples  = 512 #1024\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 1000\n",
    "\n",
    "# used by the generators and given as input to net()\n",
    "batch_size = 64\n",
    "\n",
    "# size of the Vandermonde vectors\n",
    "poly_order = 2 # Use integer for same dimension across all features\n",
    "#poly_order = [3,2,3,2,3,2,3,2,3,2] # Use list for dissimilar dimensions\n",
    "#poly_order_is_list = True # true of the polynomial order is given as a list of ints\n",
    "\n",
    "core_dim = 3 # size of the outer dimensions (mode-1 and mode-3) of the cores in the TR\n",
    "\n",
    "num_classes = 10 # corresponding to the number of digits in NMIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor ring / tensor chain\n",
    "\n",
    "Classification of the MNIST dataset using the tensorring decompsition as the model structure. It is based on MNIST.ipynb\n",
    "\n",
    "The tensor ring or tesnor chain is the same as the tensor train except the all the core tensors have the same dimensions and the 1st and Nth core tensors are also connected as illustrated in the figure.\n",
    "\n",
    "![title](ji_2019_fig26.png)\n",
    "\n",
    "Equation (11) may therefore still be used to compute f. Execpt now all G's have the same dimension.\n",
    "\n",
    "The result is then obtained by \n",
    "\n",
    "\\begin{equation}\n",
    "    f(x) = [\\mathcal{G_1} \\times_2 \\nu(x_1)] \\times_{3,1} [\\mathcal{G_2} \\times_2 \\nu(x_2)] \\times_{3,1} [\\mathcal{G_3} \\times_2 \\nu(x_3)] \\times_{3,1} \\mathcal{G_4} \\times_{3,1} \\mathcal{G_1}\n",
    "\\end{equation}\n",
    "\n",
    "In the case of three-dimensional input (num_features) $x = (x_1,x_2,x_3)^T$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load MNIST\n",
    "#To speed up training we'll only work on a subset of the data\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "# Load data from https://www.openml.org/d/554\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if perform_pca:\n",
    "    from scipy.linalg import svd\n",
    "\n",
    "    X = X.astype('float32')\n",
    "    N, M = X.shape\n",
    "\n",
    "    # Std data\n",
    "    Y = X - np.ones((N,1))*X.mean(axis=0)\n",
    "    Y /= 255.\n",
    "    ## Y = X - np.ones((N,1))*X.mean(axis=0)\n",
    "    ## Y = Y[:,np.std(Y,0) != 0]\n",
    "    ## Y = Y*(1/np.std(Y,0))\n",
    "\n",
    "    # PCA\n",
    "    U,S,V = svd(Y,full_matrices=False)\n",
    "    rho = (S*S) / (S*S).sum()\n",
    "    Z = Y@V # matrix multiplication\n",
    "    Z = Z[:,:n_PCA]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create train, val & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information on dataset\n",
      "x_train (1024, 5)\n",
      "targets_train (1024,)\n",
      "x_valid (512, 5)\n",
      "targets_valid (512,)\n",
      "x_test (512, 5)\n",
      "targets_test (512,)\n",
      "num_features:  5\n"
     ]
    }
   ],
   "source": [
    "y = y.astype('int32')\n",
    "\n",
    "if perform_pca:\n",
    "    ### Use output from PCA ###\n",
    "    random_state = check_random_state(0)\n",
    "    permutation = random_state.permutation(X.shape[0])\n",
    "    Z = Z[permutation]\n",
    "    y = y[permutation]\n",
    "    Z = Z.reshape((X.shape[0], -1))\n",
    "\n",
    "    x_train, X_test, targets_train, y_test = train_test_split(\n",
    "        Z, y, train_size=train_samples, test_size= (cv_samples+test_samples) )\n",
    "\n",
    "    x_valid, x_test, targets_valid, targets_test = train_test_split(\n",
    "        X_test, y_test, train_size=cv_samples, test_size=test_samples)\n",
    "else:\n",
    "    ### no PCA, use raw data ###\n",
    "    # Split in training, testing and validation set\n",
    "    x_train, X_test, targets_train, y_test = train_test_split(\n",
    "        X, y, train_size=train_samples, test_size=(cv_samples+test_samples))\n",
    "\n",
    "    x_valid, x_test, targets_valid, targets_test = train_test_split(\n",
    "        X_test, y_test, train_size=cv_samples, test_size=test_samples)\n",
    "\n",
    "num_features = x_train.shape[1]\n",
    "\n",
    "print(\"Information on dataset\")\n",
    "print(\"x_train\", x_train.shape)\n",
    "print(\"targets_train\", targets_train.shape)\n",
    "print(\"x_valid\", x_valid.shape)\n",
    "print(\"targets_valid\", targets_valid.shape)\n",
    "print(\"x_test\", x_test.shape)\n",
    "print(\"targets_test\", targets_test.shape)\n",
    "print('num_features: ',num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def vandermonde_vec(dataset, num_instances, num_features, poly_order, poly_order_is_list=False):\n",
    "    # Function for generating the Vandermonde vectors as a three-way array\n",
    "    \n",
    "    # The poly_order is the same across all the features\n",
    "    if poly_order_is_list == False:\n",
    "        u = np.zeros((num_instances,num_features,poly_order))\n",
    "        # u is a 3-dimensional tensor, which contains the Vandermonde vectors for every feature\n",
    "        # and every training point\n",
    "\n",
    "        # Get powers\n",
    "        for i in range(num_instances):\n",
    "            for j in range(num_features):\n",
    "                for k in range(poly_order):\n",
    "                    u[i,j,k] = np.power([dataset[i,j]], k)\n",
    "    \n",
    "    # poly_order is a list of numbers with the polynomial orders for the features\n",
    "    else:        \n",
    "        # The size of u depends on the largest poly_order\n",
    "        u = np.zeros((num_instances,num_features,max(poly_order)))\n",
    "        \n",
    "        # Get powers\n",
    "        for i in range(num_instances):\n",
    "            for j in range(num_features):\n",
    "                for k in range(max(poly_order)):\n",
    "                    # Raise the feature to the poly_order or insert 0\n",
    "                    u[i,j,k] = np.power([dataset[i,j]], k) if poly_order[j] > (k) else 0\n",
    "                     \n",
    "    #u = u.reshape(num_instances,num_features,poly_order)\n",
    "    return u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the generator\n",
    "if use_cuda:\n",
    "    params = {'batch_size': batch_size,\n",
    "              'shuffle': True,\n",
    "              'num_workers': 8,\n",
    "              'drop_last': True,\n",
    "              'pin_memory': True}\n",
    "else:\n",
    "    params = {'batch_size': batch_size,\n",
    "              'shuffle': True,\n",
    "              'num_workers': 8,\n",
    "              'drop_last': True}\n",
    "\n",
    "# Using Dataset class\n",
    "class Dataset(tutils.data.Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "    # Initialise data to be used by the other functions\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "    \n",
    "    def __len__(self):\n",
    "    # Length of the dataset, i.e. number of samples\n",
    "        return len(self.targets)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "    # Returns sample from the dataset\n",
    "        x = torch.FloatTensor(self.features[index])\n",
    "        y = self.targets[index]\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define network\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, num_features, poly_order, num_output, outer_dim, poly_order_is_list=False):\n",
    "        super(Net, self).__init__()\n",
    "        if poly_order_is_list:\n",
    "            poly_list = poly_order\n",
    "            poly_order = max(poly_order)\n",
    "        \n",
    "        self.num_features = num_features\n",
    "        self.poly_order = poly_order\n",
    "        #outer_dim = 3 # outer dimensions of the cores, G\n",
    "        \n",
    "        # Get the dimensions of the core tensors\n",
    "        # Here, they are all of the same size except the last core\n",
    "        gn_size = tuple([outer_dim,num_output,outer_dim]) # Dimension of the last cores tensor\n",
    "        gstack_size = tuple([outer_dim,poly_order,outer_dim,num_features]) # Dimension of the stack of cores\n",
    "        \n",
    "        # Elements are drawn from a uniform distribution\n",
    "        bound_i = 1/math.sqrt(outer_dim)\n",
    "        # bounds on the uniform distribution\n",
    "        lb = 0.1*bound_i\n",
    "        ub = 1.0*bound_i\n",
    "        \n",
    "        # The cores are now combined to give one long dimension which matched the one from vandermonde\n",
    "        self.Gstack = Parameter(init.uniform_(torch.empty(gstack_size, requires_grad=True),a=lb,b=ub))\n",
    "        \n",
    "        # The last tensor as a different size as the inner dimension is the number of classes\n",
    "        self.GN = Parameter(init.uniform_(torch.empty(gn_size, requires_grad=True),a=lb,b=ub))\n",
    "        \n",
    "        print('Stacked G\\'s: ', self.Gstack.shape,'\\nGN: ',self.GN.shape)\n",
    "        \n",
    "        # replace some parameters by zeros if the poly_order is not the same for all cores\n",
    "        if poly_order_is_list:\n",
    "            for j in range(num_features):\n",
    "                if poly_list[j] < (poly_order):\n",
    "                    self.Gstack[:,poly_list[j]:poly_order,:,j] = 0.0\n",
    "            self.Gstack=Parameter(self.Gstack)\n",
    "\n",
    "    def forward(self, tensor_input, batch_size, print_expr=False):\n",
    "       # Multiplication of Vandermonde vectors\n",
    "        Gv_stack= torch.einsum('abcd, edb -> aecd',self.Gstack,tensor_input)\n",
    "      \n",
    "        # Multiplication of the cores (contraction) to get f: G_i-1 x_31 G_i\n",
    "        f_stack = Gv_stack[:,:,:,0]\n",
    "        # The multiplication by the last core and the ring multiplication is not in the for-loop\n",
    "        for i in range(1,self.num_features):\n",
    "            f_stack = torch.einsum('abc, cbe -> abe',f_stack,Gv_stack[:,:,:,i])\n",
    "        \n",
    "        # Multiplication of the last core\n",
    "        f_stack = torch.einsum('abc, cda -> bd',f_stack, self.GN)\n",
    "\n",
    "        try:\n",
    "            del Gv_stack\n",
    "            torch.cuda.empty_cache()\n",
    "        except:\n",
    "            print(\"Memory not released in forward pass.\")\n",
    "            \n",
    "        #return f_stack, which should now have dimension batch_size,num_classes\n",
    "        return f_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train before vandermonde (1024, 5)\n",
      "train after vandermonde (1024, 5, 2)\n",
      "Stacked G's:  torch.Size([3, 2, 3, 5]) \n",
      "GN:  torch.Size([3, 10, 3])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-7f0f191b60eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Initialise model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoly_order\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcore_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#optimizer = optim.Adam(net.parameters())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_backward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    221\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory"
     ]
    }
   ],
   "source": [
    "# Transform input data\n",
    "num_features = x_train.shape[1]\n",
    "\n",
    "print(\"train before vandermonde\",x_train.shape)\n",
    "\n",
    "x_train = vandermonde_vec(x_train, x_train.shape[0], num_features, poly_order)\n",
    "x_valid = vandermonde_vec(x_valid, x_valid.shape[0], num_features, poly_order)\n",
    "x_test = vandermonde_vec(x_test, x_test.shape[0], num_features, poly_order)\n",
    "\n",
    "print(\"train after vandermonde\",x_train.shape)\n",
    "\n",
    "# Apply generators to the data\n",
    "training_set = Dataset(x_train, targets_train)\n",
    "training_generator = tutils.data.DataLoader(training_set, **params)\n",
    "\n",
    "validation_set = Dataset(x_valid, targets_valid)\n",
    "validation_generator = tutils.data.DataLoader(validation_set, **params)\n",
    "\n",
    "test_set = Dataset(x_test, targets_test)\n",
    "test_generator = tutils.data.DataLoader(test_set, **params)\n",
    "\n",
    "# Initialise model\n",
    "net = Net(num_features, poly_order, num_classes, core_dim).to(device)\n",
    "\n",
    "#optimizer = optim.Adam(net.parameters())\n",
    "# Optimizer with l2 regularisation\n",
    "#optimizer = optim.Adam(net.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "#optimizer = optim.LBFGS(net.parameters())\n",
    "optimizer = optim.LBFGS(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print('Norm G\\'s',torch.norm(next(net.parameters()),p='fro'))\n",
    "\n",
    "### Training the model ###\n",
    "# setting up lists for handling loss/accuracy\n",
    "train_acc, train_loss = [], []\n",
    "valid_acc, valid_loss = [], []\n",
    "test_acc, test_loss = [], []\n",
    "cur_loss = 0\n",
    "losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward -> Backprob -> Update params\n",
    "    ## Train\n",
    "    net.train()\n",
    "    cur_loss = 0\n",
    "    count = 0\n",
    "    for data, labels in training_generator:\n",
    "        count += 1\n",
    "        # Transfer training data and targets to device\n",
    "        data = data.to(device)\n",
    "        target_batch = Variable(labels.long()).to(device)\n",
    "\n",
    "        # Send it through the model\n",
    "        output = net(data, batch_size)\n",
    "\n",
    "        # compute gradients given loss\n",
    "        batch_loss = criterion(output, target_batch)\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        cur_loss += batch_loss   \n",
    "\n",
    "        # Manually release memory\n",
    "        try:\n",
    "            del data, labels, target_batch, output, batch_loss \n",
    "            torch.cuda.empty_cache()\n",
    "        except:\n",
    "            print(\"Memory not released in training.\")\n",
    "\n",
    "    losses.append(cur_loss / batch_size)\n",
    "\n",
    "    net.eval()\n",
    "    ### Evaluate training\n",
    "    train_preds, train_targs = [], []\n",
    "    for data, labels in training_generator:\n",
    "        data = data.to(device)\n",
    "\n",
    "        output = net(data, batch_size)\n",
    "        preds = torch.max(output, 1)[1]\n",
    "\n",
    "        train_targs += list(labels)\n",
    "        train_preds += list(preds.data.cpu().numpy())\n",
    "\n",
    "        # Manually release memory\n",
    "        try:\n",
    "            del data, labels, output, preds \n",
    "            torch.cuda.empty_cache()\n",
    "        except:\n",
    "            print(\"Memory not released in training evaluation.\")\n",
    "\n",
    "    ### Evaluate validation\n",
    "    val_preds, val_targs = [], []\n",
    "    for data, labels in validation_generator:\n",
    "        data = data.to(device)\n",
    "\n",
    "        output = net(data, batch_size)\n",
    "        preds = torch.max(output, 1)[1]\n",
    "        val_preds += list(preds.data.cpu().numpy())\n",
    "        val_targs += list(labels)\n",
    "\n",
    "        # Manually release memory\n",
    "        try:\n",
    "            del data, labels, output, preds\n",
    "            torch.cuda.empty_cache()\n",
    "        except:\n",
    "            print(\"Memory not released in validation.\")\n",
    "\n",
    "    train_acc_cur = accuracy_score(train_targs, train_preds)\n",
    "    valid_acc_cur = accuracy_score(val_targs, val_preds)\n",
    "\n",
    "    train_acc.append(train_acc_cur)\n",
    "    valid_acc.append(valid_acc_cur)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Epoch %2i : Train Loss %f , Train acc %f, Valid acc %f\" % (\n",
    "                epoch+1, losses[-1], train_acc_cur, valid_acc_cur))\n",
    "        print(\"CUDA memory usage in Gb: \",torch.cuda.memory_allocated()*1000**(-3))\n",
    "        print('Norm G\\'s',torch.norm(next(net.parameters()),p='fro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the parameters of the model\n",
    "i=0\n",
    "for G in net.parameters():\n",
    "    if i == 0:\n",
    "        G_net = G\n",
    "    elif i == 1:\n",
    "        GN_net = G\n",
    "    i+=1\n",
    "    \n",
    "f_net = G_net[:,:,:,0]\n",
    "for i in range(1,num_features):\n",
    "    f_net = torch.einsum('abc, cbe -> abe',f_net,G_net[:,:,:,i])\n",
    "\n",
    "f_net = torch.einsum('abc, cda -> bd',f_net, GN_net)\n",
    "\n",
    "print('Norm G\\'s',torch.norm(G_net,p='fro'))\n",
    "print('f shape: ',f_net.shape,'\\nf: ',f_net)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
