{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import copy\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "from torch.utils import data\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "from pandas import DataFrame\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from numpy import savetxt\n",
    "from numpy import loadtxt\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Cuda is available: \",torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define classification problem\n",
    "n_samples = 20640\n",
    "num_classes = 5\n",
    "std = 4\n",
    "noise = 1\n",
    "n_features = 6\n",
    "\n",
    "# save problem\n",
    "#X, y = make_blobs(n_samples, centers=num_classes, n_features=n_features, cluster_std=std)\n",
    "#savetxt('data/X_classification.csv', X, delimiter=',')\n",
    "#savetxt('data/y_classification.csv', y, delimiter=',')\n",
    "#X_tmp, X_test, y_tmp, y_test = train_test_split(X, y, test_size=0.15, random_state=0)\n",
    "#X_train, X_val, y_train, y_val = train_test_split(X_tmp, y_tmp, test_size=0.15, random_state=0)\n",
    "#savetxt('data/X_test_classification.csv', X_test, delimiter=',')\n",
    "#savetxt('data/y_test_classification.csv', y_test, delimiter=',')\n",
    "#savetxt('data/X_val_classification.csv', X_val, delimiter=',')\n",
    "#savetxt('data/y_val_classification.csv', y_val, delimiter=',')\n",
    "#savetxt('data/X_train_classification.csv', X_train, delimiter=',')\n",
    "#savetxt('data/y_train_classification.csv', y_train, delimiter=',')\n",
    "\n",
    "# fetch classification problem\n",
    "X = loadtxt('data/problem/X_classification.csv',delimiter=',')\n",
    "y = loadtxt('data/problem/y_classification.csv',delimiter=',')\n",
    "X_test = loadtxt('data/problem/X_test_classification.csv',delimiter=',')\n",
    "y_test = loadtxt('data/problem/y_test_classification.csv',delimiter=',')\n",
    "X_val = loadtxt('data/problem/X_val_classification.csv',delimiter=',')\n",
    "y_val = loadtxt('data/problem/y_val_classification.csv',delimiter=',')\n",
    "X_train = loadtxt('data/problem/X_train_classification.csv',delimiter=',')\n",
    "y_train = loadtxt('data/problem/y_train_classification.csv',delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot hyperplane of problem\n",
    "def plot_classification_problem(X, y, n_features, dim1=0, dim2=1):\n",
    "    assert dim1 < n_features, \"cannot plot in higher dimensions than the problem\"\n",
    "    assert dim2 < n_features, \"cannot plot in higher dimensions than the problem\"\n",
    "    \n",
    "    df = DataFrame(dict(x=X[:,dim1], y=X[:,dim2], label=y))\n",
    "    colors = {0:'red', 1:'blue', 2:'green', 3:'pink', 4:'cyan',5:'yellow',6:'black',7:'gray'}\n",
    "    fig, ax = plt.subplots()\n",
    "    grouped = df.groupby('label')\n",
    "    for key, group in grouped:\n",
    "        group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])\n",
    "    plt.show()\n",
    "\n",
    "plot_classification_problem(X_train, y_train, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the 2d vandermonde vectors (will be reshaped to 3d later when necessary)\n",
    "def vandermonde_vec(dataset, n_instances, n_features, poly_order):\n",
    "    u = np.zeros((n_instances, n_features*poly_order))\n",
    "    \n",
    "    # Get powers\n",
    "    for row in range(n_instances):\n",
    "        for col in range(n_features):\n",
    "            u[row,col*poly_order:(col+1)*poly_order] = np.power(\n",
    "                [dataset[row,col]]*poly_order, list(range(poly_order)))\n",
    "    return u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TTNet(nn.Module):\n",
    "\n",
    "    def __init__(self, n_features, poly_order, num_output, rank):\n",
    "        super(TTNet, self).__init__()  \n",
    "        \n",
    "        self.n_features = n_features\n",
    "        self.poly_order = poly_order\n",
    "        self.num_output = num_output\n",
    "        self.rank = rank\n",
    "        self.type = 'tt'\n",
    "\n",
    "        Di = self.rank\n",
    "        Dn = self.rank\n",
    "        # Elements are drawn from a uniform distribution [-1/sqrt(D),1/sqrt(D)]\n",
    "        bound_i = 1/np.sqrt(Di)\n",
    "        bound_n = 1/np.sqrt(Dn)\n",
    "        # bounds on the uniform distribution\n",
    "        lb = 0.5*bound_i\n",
    "        ub = 1.0*bound_i\n",
    "\n",
    "        # input layer\n",
    "        self.tt_cores = []\n",
    "        for i in range(n_features):\n",
    "            if i==0: \n",
    "                tn_size = (1,poly_order,self.rank)\n",
    "            elif i==n_features-1:\n",
    "                tn_size = (self.rank,poly_order,num_output)\n",
    "            else:\n",
    "                tn_size = (self.rank,poly_order,self.rank)\n",
    "            \n",
    "            k = 1/(np.sqrt(self.poly_order))\n",
    "            g_i = Parameter(init.normal_(torch.empty(tn_size, requires_grad=True), mean=0, std=1)*k)\n",
    "            self.tt_cores.append(g_i)\n",
    "            \n",
    "        self.tt_cores = nn.ParameterList(self.tt_cores)\n",
    "\n",
    "    def get_n_params(self):\n",
    "        pp=0\n",
    "        for p in list(self.parameters()):\n",
    "            nn=1\n",
    "            for s in list(p.size()):\n",
    "                nn = nn*s\n",
    "            pp += nn\n",
    "        return pp\n",
    "\n",
    "    def forward(self, vec_input, batch_size, print_expr=False):\n",
    "                \n",
    "        vec = vec_input[:,:self.poly_order].reshape(batch_size,-1)\n",
    "        \n",
    "        # First do: G_i x_2 v_i\n",
    "        mode2 = []\n",
    "        for i in range(self.n_features):\n",
    "            vec = vec_input[:,i*self.poly_order:(i+1)*self.poly_order].reshape(batch_size,-1)\n",
    "            mode2.append(torch.einsum('abc,db -> dac', self.tt_cores[i], vec) )\n",
    "            \n",
    "        mode2[0] = mode2[0].reshape(batch_size,self.rank)\n",
    "        mode2[-1] = mode2[-1].reshape(batch_size,self.rank,self.num_output)\n",
    "        \n",
    "        # Join all the results (based on equation 11 in the paper)\n",
    "        result = mode2[0]\n",
    "        for i in range(self.n_features-1):\n",
    "            result = torch.einsum('ab,abd -> ad', result, mode2[i+1]) * 1/self.rank\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TRNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_features, poly_order, num_output, rank):\n",
    "        super(TRNet, self).__init__()  \n",
    "        \n",
    "        self.n_features = n_features\n",
    "        self.poly_order = poly_order\n",
    "        self.num_output = num_output\n",
    "        self.rank = rank\n",
    "        self.outer_dim = rank # outer dimensions of the cores, G\n",
    "        self.type = 'tr'\n",
    "        \n",
    "        # Specify the dimensions of the core tensors\n",
    "        # Here, they are all of order 3 and the dimension of the first and last mode is 3\n",
    "        gi_size = tuple([self.outer_dim,poly_order,self.outer_dim]) # Dimension of the core tensors except the last\n",
    "        gn_size = tuple([self.outer_dim,num_output,self.outer_dim]) # Dimension of the last cores tensor\n",
    "        gstack_size = tuple([self.outer_dim,poly_order,self.outer_dim,n_features]) # Dimension of the stack of cores\n",
    "        \n",
    "        # Elements are drawn from a uniform distribution [-1/sqrt(D),1/sqrt(D)],\n",
    "        # where D is the outer dimensions of the core\n",
    "        bound_i = 1/math.sqrt(self.outer_dim)\n",
    "        # bounds on the uniform distribution\n",
    "        lb = 0.1*bound_i\n",
    "        ub = 1.0*bound_i\n",
    "        \n",
    "        # The cores are now combined to give one long dimension which matched the one from vandermonde\n",
    "        self.Gstack = Parameter(init.uniform_(torch.empty(gstack_size, requires_grad=True),a=lb,b=ub))\n",
    "        \n",
    "        # The last tensor as a different size as the inner dimension is the number of classes\n",
    "        self.GN = Parameter(init.uniform_(torch.empty(gn_size, requires_grad=True),a=lb,b=ub))\n",
    "\n",
    "    def forward(self, tensor_input, batch_size, print_expr=False):\n",
    "        \n",
    "        # Multiplication of Vandermonde vectors\n",
    "        Gv_stack= torch.einsum('abcd, edb -> aecd',self.Gstack,tensor_input)\n",
    "        \n",
    "        # Multiplication of the cores to get f: G_i-1 x_31 G_i\n",
    "        f_stack = Gv_stack[:,:,:,0]\n",
    "        \n",
    "        # The multiplication by the last core and the ring multiplication is not in the for-loop\n",
    "        for i in range(1,self.n_features):\n",
    "            f_stack = torch.einsum('abc, cbe -> abe',f_stack,Gv_stack[:,:,:,i])\n",
    "\n",
    "        f_stack = torch.einsum('abc, cda -> bd',f_stack, self.GN)\n",
    "        return f_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPNet(nn.Module):\n",
    "\n",
    "    def __init__(self, n_features, poly_order, num_output, rank):\n",
    "        super(CPNet, self).__init__()  \n",
    "        \n",
    "        self.rank = rank\n",
    "        self.n_features = n_features\n",
    "        self.poly_order = poly_order\n",
    "        self.num_output = num_output\n",
    "        self.type = 'cp'\n",
    "        \n",
    "        # weight tensors collected in one tensor\n",
    "        tn_size = tuple([n_features] + [poly_order] + [rank] + [num_output]) # size of all tensors A_i\n",
    "        self.A = Parameter(init.normal_(torch.empty(tn_size, requires_grad=True), std=0.575))\n",
    "\n",
    "    def forward(self, vec_input, batch_size, print_expr=False):\n",
    "        \n",
    "        m = torch.einsum('abcd,eab->aced',self.A ,vec_input)\n",
    "        f = torch.prod(m,0)\n",
    "        \n",
    "        return torch.sum(f,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train classification model\n",
    "def train_classification_model(net, optimizer, criterion, num_epochs, n_features, X_train, X_val, y_train, y_val):\n",
    "    # Training the model\n",
    "    n_train_samples = len(X_train[:,0])\n",
    "    n_val_samples = len(X_val[:,0])\n",
    "    \n",
    "    X_train = vandermonde_vec(X_train, n_train_samples, n_features, net.poly_order)\n",
    "    X_val = vandermonde_vec(X_val, n_val_samples, n_features, net.poly_order)\n",
    "    \n",
    "    # reshape X_train, X_val to 3d vandermonde for tensor ring and cpd\n",
    "    if (net.type == 'tr') or (net.type == 'cp'):\n",
    "        X_train = np.reshape(X_train, (n_train_samples, n_features, net.poly_order))\n",
    "        X_val = np.reshape(X_val, (n_val_samples, n_features, net.poly_order))\n",
    "    \n",
    "    # setting up lists for handling loss/accuracy\n",
    "    train_acc = np.zeros(num_epochs)\n",
    "    valid_acc = np.zeros(num_epochs)\n",
    "    losses = -1*np.ones(num_epochs)\n",
    "    loss = 0\n",
    "    valid_acc_cur_best = 0\n",
    "    best_model = copy.deepcopy(net)\n",
    "    min_val_output = np.zeros(n_val_samples)\n",
    "    min_val_preds = np.zeros(n_val_samples)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward -> Backprob -> Update params\n",
    "        ### Train\n",
    "        net.train()\n",
    "        loss = 0\n",
    "        count = 0\n",
    "\n",
    "        # Get data\n",
    "        data = torch.Tensor(X_train)\n",
    "        targets = torch.Tensor(y_train)\n",
    "        count += 1\n",
    "\n",
    "        # Transfer training data and targets to device\n",
    "        data = data.to(device)\n",
    "        targets = Variable(targets.long()).to(device)\n",
    "        \n",
    "        # Send it through the model\n",
    "        output = net(data, n_train_samples)\n",
    "        \n",
    "        # compute gradients given loss\n",
    "        loss = criterion(output, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        #optimizer.step()\n",
    "        \n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            output = net(data, n_train_samples)\n",
    "            loss = criterion(output, targets)\n",
    "            loss.backward()\n",
    "            return loss\n",
    "        optimizer.step(closure)\n",
    "        losses[epoch] = loss.detach().cpu().numpy()\n",
    "        \n",
    "        #eval\n",
    "        net.eval()\n",
    "\n",
    "        ### Evaluate training\n",
    "        train_preds, train_targs = [], []\n",
    "\n",
    "        #for data, labels in training_generator:\n",
    "        train_data = torch.Tensor(X_train)\n",
    "        train_targets = torch.Tensor(y_train)\n",
    "        train_data = train_data.to(device)\n",
    "        \n",
    "        ### classification\n",
    "        train_output = net(train_data, n_train_samples)\n",
    "        preds = torch.max(train_output, 1)[1]\n",
    "        \n",
    "        train_preds += list(preds.data.cpu().numpy())\n",
    "        train_targs += list(train_targets)\n",
    "        \n",
    "        ### Evaluate validation\n",
    "        val_preds, val_targs = [], []\n",
    "\n",
    "        #for data, labels in validation_generator:\n",
    "        val_data = torch.Tensor(X_val)\n",
    "        val_targets = torch.Tensor(y_val)\n",
    "        val_data = val_data.to(device)\n",
    "\n",
    "        ### classification\n",
    "        val_output = net(val_data,n_val_samples)\n",
    "        preds = torch.max(val_output, 1)[1]\n",
    "        \n",
    "        val_preds += list(preds.data.cpu().numpy())\n",
    "        val_targs += list(val_targets)\n",
    "        \n",
    "        if np.isnan(losses[epoch]):\n",
    "            print(\"NaN encountered.\")\n",
    "            return best_model, train_acc, valid_acc, losses, valid_acc_cur_best, min_val_output, min_val_preds\n",
    "\n",
    "        train_acc_cur = accuracy_score(train_targs, train_preds)\n",
    "        valid_acc_cur = accuracy_score(val_targs, val_preds)\n",
    "        \n",
    "        if valid_acc_cur > valid_acc_cur_best:\n",
    "            best_model = copy.deepcopy(net)\n",
    "            valid_acc_cur_best = valid_acc_cur\n",
    "            min_val_output = val_output.data.cpu().numpy()\n",
    "            min_val_preds = preds.data.cpu().numpy()\n",
    "\n",
    "        train_acc[epoch] = train_acc_cur\n",
    "        valid_acc[epoch] = valid_acc_cur\n",
    "        \n",
    "        print(\"Epoch %2i : Train Loss %f , Train acc %f, Valid acc %f\" % (\n",
    "            epoch+1, losses[epoch], train_acc_cur, valid_acc_cur))\n",
    "\n",
    "        #if epoch % (np.floor(num_epochs/10)) == 0:\n",
    "        #    print(\"Epoch %2i : Train Loss %f , Train acc %f, Valid acc %f\" % (\n",
    "        #        epoch+1, losses[-1], train_acc_cur, valid_acc_cur))\n",
    "\n",
    "        #if epoch == num_epochs-1:\n",
    "        #    print(\"Last epoch %2i : Final Train Acc %f\" % (epoch+1, train_acc_cur))\n",
    "    \n",
    "    return best_model, train_acc, valid_acc, losses, valid_acc_cur_best, min_val_output, min_val_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Universal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_orders = np.arange(4,6) #[1,2,3]\n",
    "ranks = np.arange(1,16) #[1,...25]\n",
    "num_output = num_classes\n",
    "\n",
    "#tensor train classification\n",
    "models = []\n",
    "for poly_order in poly_orders:\n",
    "    for rank in ranks:\n",
    "        model = TTNet(n_features, poly_order, num_output, rank)\n",
    "        model.to(device)\n",
    "        models.append(model)\n",
    "\n",
    "#train the different models\n",
    "num_epochs = 200\n",
    "losses_models = np.zeros((len(models), num_epochs))\n",
    "train_acc_models = np.zeros((len(models), num_epochs))\n",
    "valid_acc_models = np.zeros((len(models), num_epochs))\n",
    "\n",
    "for i in range(len(models)):\n",
    "    print(\"Model:\",i+1)\n",
    "    model = models[i]\n",
    "    #optimizer = optim.Adam(model.parameters())\n",
    "    optimizer = optim.LBFGS(model.parameters(), line_search_fn='strong_wolfe')\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    models[i], train_acc, valid_acc, losses, valid_acc_best = train_classification_model(\n",
    "        model, optimizer, criterion, num_epochs, n_features, X_train, X_val, y_train, y_val)\n",
    "    losses_models[i] = losses\n",
    "    train_acc_models[i] = train_acc\n",
    "    valid_acc_models[i] = valid_acc\n",
    "\n",
    "# save to csv file\n",
    "savetxt('data/overfitting_poly_orders.csv', poly_orders, delimiter=',')\n",
    "savetxt('data/overfitting_ranks.csv', ranks, delimiter=',')\n",
    "savetxt('data/overfitting_train_acc_models.csv', train_acc_models, delimiter=',')\n",
    "savetxt('data/overfitting_losses_models.csv', losses_models, delimiter=',')\n",
    "savetxt('data/overfitting_valid_acc_models.csv', valid_acc_models, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossvalidation_classification(models, K, X_train, X_val, y_train, y_val):\n",
    "    S = len(models)\n",
    "    \n",
    "    CV = model_selection.KFold(n_splits=K, shuffle=True, random_state=0)\n",
    "    \n",
    "    # observations to do CV on\n",
    "    X_CV = np.concatenate((X_train, X_val))\n",
    "    y_CV = np.concatenate((y_train, y_val))\n",
    "    \n",
    "    opt_models = []\n",
    "    max_accs = []\n",
    "    \n",
    "    valid_accs = -1*np.ones(K*S)\n",
    "    num_epochs = 100\n",
    "    \n",
    "    k=0\n",
    "    for par_index, test_index in CV.split(X_CV):\n",
    "        print('Computing CV fold: {0}/{1}..'.format(k+1,K))\n",
    "        \n",
    "        # extract training and test set for current CV fold\n",
    "        X_tr, y_tr = X_CV[par_index,:], y_CV[par_index]\n",
    "        X_va, y_va = X_CV[test_index,:], y_CV[test_index]\n",
    "        \n",
    "        max_acc = 0\n",
    "        opt_model = None\n",
    "        \n",
    "        for s in range(S):\n",
    "            model = models[s]\n",
    "            \n",
    "            poly_order = model.poly_order\n",
    "            \n",
    "            # define optimizer and loss criterion\n",
    "            optimizer = optim.LBFGS(model.parameters())\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "                \n",
    "            # train net\n",
    "            model_trained, train_acc, valid_acc, losses, valid_acc_cur_best = train_classification_model(\n",
    "                model, optimizer, criterion, num_epochs, n_features, X_tr, X_va, y_tr, y_va)\n",
    "            \n",
    "            valid_accs[k*S+s] = valid_acc_cur_best\n",
    "            \n",
    "            acc = valid_acc_cur_best\n",
    "            if acc > max_acc:\n",
    "                max_acc = acc\n",
    "                opt_model = model_trained\n",
    "                \n",
    "        opt_models.append(opt_model)\n",
    "        max_accs.append(max_acc)\n",
    "        k+=1\n",
    "    return opt_models, max_accs, valid_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV TT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = loadtxt('data/cv_ranks.csv',delimiter=',')#range(3,15)\n",
    "poly_orders = loadtxt('data/cv_poly_orders.csv',delimiter=',')#range(1,4)\n",
    "models = []\n",
    "for rank in ranks:\n",
    "    for poly_order in poly_orders:\n",
    "        net = TTNet(int(n_features), int(poly_order), int(num_output), int(rank))\n",
    "        net.to(device)\n",
    "        models.append(net)\n",
    "\n",
    "K = 3\n",
    "opt_models, opt_models_max_accs, valid_accs = crossvalidation_classification(\n",
    "    models, K, X_train, X_val, y_train, y_val)\n",
    "\n",
    "#savetxt('data/cv_ttcls_valid_accs.csv',valid_accs,delimiter=',')\n",
    "\n",
    "opt_models_poly_order = []\n",
    "opt_models_rank = []\n",
    "for i in range(K):\n",
    "    opt_models_poly_order.append(opt_models[i].poly_order)\n",
    "    opt_models_rank.append(opt_models[i].rank)\n",
    "    print('optimal poly_order:', opt_models[i].poly_order)\n",
    "    print('optimal rank:', opt_models[i].rank)\n",
    "    print('minimal validation accuracy of optimal model:', opt_models_max_accs[i])\n",
    "    \n",
    "#savetxt('data/cv_ttcls_opt_models_poly_order.csv', opt_models_poly_order,delimiter=',')\n",
    "#savetxt('data/cv_ttcls_opt_models_rank.csv', opt_models_rank,delimiter=',')\n",
    "#savetxt('data/cv_ttcls_opt_models_max_accs.csv', opt_models_max_accs,delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV TR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = loadtxt('data/cv_ranks.csv',delimiter=',')#range(3,15)\n",
    "poly_orders = loadtxt('data/cv_poly_orders.csv',delimiter=',')#range(1,4)\n",
    "models = []\n",
    "for rank in ranks:\n",
    "    for poly_order in poly_orders:\n",
    "        net = TRNet(int(n_features), int(poly_order), int(num_output), int(rank))\n",
    "        net.to(device)\n",
    "        models.append(net)\n",
    "\n",
    "K = 3\n",
    "opt_models, opt_models_max_accs, valid_accs = crossvalidation_classification(\n",
    "    models, K, X_train, X_val, y_train, y_val)\n",
    "\n",
    "#savetxt('data/cv_trcls_valid_accs.csv',valid_accs,delimiter=',')\n",
    "\n",
    "opt_models_poly_order = []\n",
    "opt_models_rank = []\n",
    "for i in range(K):\n",
    "    opt_models_poly_order.append(opt_models[i].poly_order)\n",
    "    opt_models_rank.append(opt_models[i].rank)\n",
    "    print('optimal poly_order:', opt_models[i].poly_order)\n",
    "    print('optimal rank:', opt_models[i].rank)\n",
    "    print('minimal validation accuracy of optimal model:', opt_models_max_accs[i])\n",
    "    \n",
    "#savetxt('data/cv_trcls_opt_models_poly_order.csv', opt_models_poly_order,delimiter=',')\n",
    "#savetxt('data/cv_trcls_opt_models_rank.csv', opt_models_rank,delimiter=',')\n",
    "#savetxt('data/cv_trcls_opt_models_max_accs.csv', opt_models_max_accs,delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV CP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = loadtxt('data/cv_ranks.csv',delimiter=',')#range(3,15)\n",
    "poly_orders = loadtxt('data/cv_poly_orders.csv',delimiter=',')#range(1,4)\n",
    "models = []\n",
    "for rank in ranks:\n",
    "    for poly_order in poly_orders:\n",
    "        net = TRNet(int(n_features), int(poly_order), int(num_output), int(rank))\n",
    "        net.to(device)\n",
    "        models.append(net)\n",
    "\n",
    "K = 3\n",
    "opt_models, opt_models_max_accs, valid_accs = crossvalidation_classification(\n",
    "    models, K, X_train, X_val, y_train, y_val)\n",
    "\n",
    "#savetxt('data/cv_cpcls_valid_accs.csv',valid_accs,delimiter=',')\n",
    "\n",
    "opt_models_poly_order = []\n",
    "opt_models_rank = []\n",
    "for i in range(K):\n",
    "    opt_models_poly_order.append(opt_models[i].poly_order)\n",
    "    opt_models_rank.append(opt_models[i].rank)\n",
    "    print('optimal poly_order:', opt_models[i].poly_order)\n",
    "    print('optimal rank:', opt_models[i].rank)\n",
    "    print('minimal validation accuracy of optimal model:', opt_models_max_accs[i])\n",
    "    \n",
    "#savetxt('data/cv_cpcls_opt_models_poly_order.csv', opt_models_poly_order,delimiter=',')\n",
    "#savetxt('data/cv_cpcls_opt_models_rank.csv', opt_models_rank,delimiter=',')\n",
    "#savetxt('data/cv_cpcls_opt_models_max_accs.csv', opt_models_max_accs,delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run optimal models and compare with test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "% TT Cls\n",
    "% poly order 2\n",
    "% rank 10\n",
    "% optacc 90.25307797537619470 percent\n",
    "\n",
    "% TR Cls\n",
    "% poly order 2\n",
    "% rank 4\n",
    "% optacc 90.30437756497947666 percent\n",
    "\n",
    "% CP Cls\n",
    "% poly order 2\n",
    "% rank 3\n",
    "% optacc 90.23597811217510811 percent'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check test set\n",
    "def classification_test(net, X_test, y_test, n_features):\n",
    "    n_test_samples = len(X_test[:,0])\n",
    "    X_test = vandermonde_vec(X_test, n_test_samples, n_features, net.poly_order)\n",
    "    # reshape X_train, X_val to 3d vandermonde for tensor ring and cpd\n",
    "    if (net.type == 'tr') or (net.type == 'cp'):\n",
    "        X_test = np.reshape(X_test, (n_test_samples, n_features, net.poly_order))\n",
    "    \n",
    "    data = torch.Tensor(X_test)\n",
    "    targets = torch.Tensor(y_test)\n",
    "    targets = Variable(targets.long())\n",
    "    output = net(data, n_test_samples)\n",
    "    preds = torch.max(output, 1)[1]\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    loss = criterion(output, targets)\n",
    "\n",
    "    cmat = confusion_matrix(targets, preds)\n",
    "    accscore = accuracy_score(targets, preds)\n",
    "    \n",
    "    print(\"\\nConfusion matrix:\\n\", cmat)\n",
    "    print(\"\\nTest set accuracy score:\", accscore)\n",
    "    print(\"\\nLoss:\", loss)\n",
    "    #for name, param in net.named_parameters():\n",
    "        #if param.requires_grad:\n",
    "            #print(name)\n",
    "            #print(param.data.size())\n",
    "            #print(param.data)\n",
    "    \n",
    "    return accscore, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TT\n",
    "num_epochs = 100\n",
    "poly_order = 2\n",
    "rank = 10\n",
    "num_output = num_classes\n",
    "\n",
    "net = TTNet(int(n_features), int(poly_order), int(num_output), int(rank))\n",
    "optimizer = optim.LBFGS(net.parameters(), line_search_fn='strong_wolfe')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#net.to(device)\n",
    "model_trained, train_acc, valid_acc, losses, valid_acc_cur_best, min_val_output, min_val_preds = train_classification_model(\n",
    "                net, optimizer, criterion, num_epochs, n_features, X_train, X_val, y_train, y_val)\n",
    "#savetxt('data/min_val_output_tt_classification.csv', min_val_output, delimiter=',')\n",
    "#savetxt('data/min_val_preds_tt_classification.csv', min_val_preds, delimiter=',')\n",
    "#accscore, loss = classification_test(model_trained, X_test, y_test, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TR\n",
    "num_epochs = 100\n",
    "poly_order = 2\n",
    "rank = 4\n",
    "num_output = num_classes\n",
    "\n",
    "net = TRNet(n_features, poly_order, num_output, rank)\n",
    "optimizer = optim.LBFGS(net.parameters(), line_search_fn='strong_wolfe')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model_trained, train_acc, valid_acc, losses, valid_acc_cur_best, min_val_output, min_val_preds = train_classification_model(\n",
    "                net, optimizer, criterion, num_epochs, n_features, X_train, X_val, y_train, y_val)\n",
    "#savetxt('data/min_val_output_tr_classification.csv', min_val_output, delimiter=',')\n",
    "#savetxt('data/min_val_preds_tr_classification.csv', min_val_preds, delimiter=',')\n",
    "#accscore, loss = classification_test(model_trained, X_test, y_test, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CP\n",
    "num_epochs = 100\n",
    "poly_order = 2\n",
    "rank = 3\n",
    "num_output = num_classes\n",
    "\n",
    "net = CPNet(int(n_features), int(poly_order), int(num_output), int(rank))\n",
    "optimizer = optim.LBFGS(net.parameters(), line_search_fn='strong_wolfe')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model_trained, train_acc, valid_acc, losses, valid_acc_cur_best, min_val_output, min_val_preds = train_classification_model(\n",
    "                net, optimizer, criterion, num_epochs, n_features, X_train, X_val, y_train, y_val)\n",
    "\n",
    "#savetxt('data/min_val_output_cp_classification.csv', min_val_output, delimiter=',')\n",
    "#savetxt('data/min_val_preds_cp_classification.csv', min_val_preds, delimiter=',')\n",
    "#accscore, loss = classification_test(model_trained, X_test, y_test, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
