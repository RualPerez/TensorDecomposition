{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import copy\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "from torch.utils import data\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "from pandas import DataFrame\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import model_selection\n",
    "\n",
    "from numpy import savetxt\n",
    "from numpy import loadtxt\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Cuda is available: \",torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cal_housing = fetch_california_housing()\n",
    "#X = cal_housing.data\n",
    "sigma = cal_housing.target.std()**2\n",
    "feature_names = cal_housing.feature_names\n",
    "n_features = len(feature_names)\n",
    "num_output = 1\n",
    "#y -= y.mean()\n",
    "#y /= y.std()\n",
    "#scaler = StandardScaler()\n",
    "#X = scaler.fit_transform(X, y)\n",
    "\n",
    "# save problem\n",
    "#savetxt('data/X_regression.csv', X, delimiter=',')\n",
    "#savetxt('data/y_regression.csv', y, delimiter=',')\n",
    "#X_tmp, X_test, y_tmp, y_test = train_test_split(X, y, test_size=0.15, random_state=0)\n",
    "#X_train, X_val, y_train, y_val = train_test_split(X_tmp, y_tmp, test_size=0.15, random_state=0)\n",
    "#savetxt('data/X_test_regression.csv', X_test, delimiter=',')\n",
    "#savetxt('data/y_test_regression.csv', y_test, delimiter=',')\n",
    "#savetxt('data/X_val_regression.csv', X_val, delimiter=',')\n",
    "#savetxt('data/y_val_regression.csv', y_val, delimiter=',')\n",
    "#savetxt('data/X_train_regression.csv', X_train, delimiter=',')\n",
    "#savetxt('data/y_train_regression.csv', y_train, delimiter=',')\n",
    "\n",
    "# fetch classification problem\n",
    "X = loadtxt('data/problem/X_regression.csv',delimiter=',')\n",
    "y = loadtxt('data/problem/y_regression.csv',delimiter=',')\n",
    "X_test = loadtxt('data/problem/X_test_regression.csv',delimiter=',')\n",
    "y_test = loadtxt('data/problem/y_test_regression.csv',delimiter=',')\n",
    "X_val = loadtxt('data/problem/X_val_regression.csv',delimiter=',')\n",
    "y_val = loadtxt('data/problem/y_val_regression.csv',delimiter=',')\n",
    "X_train = loadtxt('data/problem/X_train_regression.csv',delimiter=',')\n",
    "y_train = loadtxt('data/problem/y_train_regression.csv',delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the 2d vandermonde vectors (will be reshaped to 3d later when necessary)\n",
    "def vandermonde_vec(dataset, n_instances, n_features, poly_order):\n",
    "    u = np.zeros((n_instances, n_features*poly_order))\n",
    "    \n",
    "    # Get powers\n",
    "    for row in range(n_instances):\n",
    "        for col in range(n_features):\n",
    "            u[row,col*poly_order:(col+1)*poly_order] = np.power(\n",
    "                [dataset[row,col]]*poly_order, list(range(poly_order)))\n",
    "    return u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TTNet(nn.Module):\n",
    "\n",
    "    def __init__(self, n_features, poly_order, num_output, rank):\n",
    "        super(TTNet, self).__init__()  \n",
    "        \n",
    "        self.n_features = n_features\n",
    "        self.poly_order = poly_order\n",
    "        self.num_output = num_output\n",
    "        self.rank = rank\n",
    "        self.type = 'tt'\n",
    "\n",
    "        Di = self.rank\n",
    "        Dn = self.rank\n",
    "        # Elements are drawn from a uniform distribution [-1/sqrt(D),1/sqrt(D)]\n",
    "        bound_i = 1/np.sqrt(Di)\n",
    "        bound_n = 1/np.sqrt(Dn)\n",
    "        # bounds on the uniform distribution\n",
    "        lb = 0.5*bound_i\n",
    "        ub = 1.0*bound_i\n",
    "\n",
    "        # input layer\n",
    "        self.tt_cores = []\n",
    "        for i in range(n_features):\n",
    "            if i==0: \n",
    "                tn_size = (1,poly_order,self.rank)\n",
    "            elif i==n_features-1:\n",
    "                tn_size = (self.rank,poly_order,num_output)\n",
    "            else:\n",
    "                tn_size = (self.rank,poly_order,self.rank)\n",
    "            \n",
    "            k = 1/(np.sqrt(self.poly_order))\n",
    "            g_i = Parameter(init.normal_(torch.empty(tn_size, requires_grad=True), mean=0, std=1)*k)\n",
    "            self.tt_cores.append(g_i)\n",
    "            \n",
    "        self.tt_cores = nn.ParameterList(self.tt_cores)\n",
    "\n",
    "    def get_n_params(self):\n",
    "        pp=0\n",
    "        for p in list(self.parameters()):\n",
    "            nn=1\n",
    "            for s in list(p.size()):\n",
    "                nn = nn*s\n",
    "            pp += nn\n",
    "        return pp\n",
    "\n",
    "    def forward(self, vec_input, batch_size, print_expr=False):\n",
    "                \n",
    "        vec = vec_input[:,:self.poly_order].reshape(batch_size,-1)\n",
    "        \n",
    "        # First do: G_i x_2 v_i\n",
    "        mode2 = []\n",
    "        for i in range(self.n_features):\n",
    "            vec = vec_input[:,i*self.poly_order:(i+1)*self.poly_order].reshape(batch_size,-1)\n",
    "            mode2.append(torch.einsum('abc,db -> dac', self.tt_cores[i], vec) )\n",
    "            \n",
    "        mode2[0] = mode2[0].reshape(batch_size,self.rank)\n",
    "        mode2[-1] = mode2[-1].reshape(batch_size,self.rank,self.num_output)\n",
    "        \n",
    "        # Join all the results (based on equation 11 in the paper)\n",
    "        result = mode2[0]\n",
    "        for i in range(self.n_features-1):\n",
    "            result = torch.einsum('ab,abd -> ad', result, mode2[i+1]) * 1/self.rank\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor ring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TRNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_features, poly_order, num_output, rank):\n",
    "        super(TRNet, self).__init__()  \n",
    "        \n",
    "        self.n_features = n_features\n",
    "        self.poly_order = poly_order\n",
    "        self.num_output = num_output\n",
    "        self.rank = rank\n",
    "        self.outer_dim = rank # outer dimensions of the cores, G\n",
    "        self.type = 'tr'\n",
    "        \n",
    "        # Specify the dimensions of the core tensors\n",
    "        # Here, they are all of order 3 and the dimension of the first and last mode is 3\n",
    "        gi_size = tuple([self.outer_dim,poly_order,self.outer_dim]) # Dimension of the core tensors except the last\n",
    "        gn_size = tuple([self.outer_dim,num_output,self.outer_dim]) # Dimension of the last cores tensor\n",
    "        gstack_size = tuple([self.outer_dim,poly_order,self.outer_dim,n_features]) # Dimension of the stack of cores\n",
    "        \n",
    "        # Elements are drawn from a uniform distribution [-1/sqrt(D),1/sqrt(D)],\n",
    "        # where D is the outer dimensions of the core\n",
    "        bound_i = 1/math.sqrt(self.outer_dim)\n",
    "        # bounds on the uniform distribution\n",
    "        lb = 0.1*bound_i\n",
    "        ub = 1.0*bound_i\n",
    "        \n",
    "        # The cores are now combined to give one long dimension which matched the one from vandermonde\n",
    "        self.Gstack = Parameter(init.uniform_(torch.empty(gstack_size, requires_grad=True),a=lb,b=ub))\n",
    "        \n",
    "        # The last tensor as a different size as the inner dimension is the number of classes\n",
    "        self.GN = Parameter(init.uniform_(torch.empty(gn_size, requires_grad=True),a=lb,b=ub))\n",
    "\n",
    "    def forward(self, tensor_input, batch_size, print_expr=False):\n",
    "        \n",
    "        # Multiplication of Vandermonde vectors\n",
    "        Gv_stack= torch.einsum('abcd, edb -> aecd',self.Gstack,tensor_input)\n",
    "        \n",
    "        # Multiplication of the cores to get f: G_i-1 x_31 G_i\n",
    "        f_stack = Gv_stack[:,:,:,0]\n",
    "        \n",
    "        # The multiplication by the last core and the ring multiplication is not in the for-loop\n",
    "        for i in range(1,self.n_features):\n",
    "            f_stack = torch.einsum('abc, cbe -> abe',f_stack,Gv_stack[:,:,:,i])\n",
    "\n",
    "        f_stack = torch.einsum('abc, cda -> bd',f_stack, self.GN)\n",
    "        return f_stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPNet(nn.Module):\n",
    "\n",
    "    def __init__(self, n_features, poly_order, num_output, rank):\n",
    "        super(CPNet, self).__init__()  \n",
    "        \n",
    "        self.rank = rank\n",
    "        self.n_features = n_features\n",
    "        self.poly_order = poly_order\n",
    "        self.num_output = num_output\n",
    "        self.type = 'cp'\n",
    "        \n",
    "        # weight tensors collected in one tensor\n",
    "        tn_size = tuple([n_features] + [poly_order] + [rank] + [num_output]) # size of all tensors A_i\n",
    "        self.A = Parameter(init.normal_(torch.empty(tn_size, requires_grad=True), std=0.575))\n",
    "\n",
    "    def forward(self, vec_input, batch_size, print_expr=False):\n",
    "        \n",
    "        m = torch.einsum('abcd,eab->aced',self.A ,vec_input)\n",
    "        f = torch.prod(m,0)\n",
    "        \n",
    "        return torch.sum(f,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train regression model\n",
    "def train_regression_model(net, optimizer, criterion, num_epochs, n_features, X_train, X_val, y_train, y_val):\n",
    "    \n",
    "    n_train_samples = len(X_train[:,0])\n",
    "    n_val_samples = len(X_val[:,0])\n",
    "    \n",
    "    # Training the model\n",
    "    X_train = vandermonde_vec(X_train, n_train_samples, n_features, net.poly_order)\n",
    "    X_val = vandermonde_vec(X_val, n_val_samples, n_features, net.poly_order)\n",
    "    \n",
    "    # reshape X_train, X_val to 3d vandermonde for tensor ring and cpd\n",
    "    if (net.type == 'tr') or (net.type == 'cp'):\n",
    "        X_train = np.reshape(X_train, (n_train_samples, n_features, net.poly_order))\n",
    "        X_val = np.reshape(X_val, (n_val_samples, n_features, net.poly_order))\n",
    "    \n",
    "    # setting up lists for handling loss/accuracy\n",
    "    train_err = -1*np.ones(num_epochs)\n",
    "    valid_err = -1*np.ones(num_epochs)\n",
    "    loss = 0\n",
    "    losses = []\n",
    "    valid_err_cur_best = 1e10\n",
    "    min_val_output = np.zeros(n_val_samples)\n",
    "    best_model = copy.deepcopy(net)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward -> Backprob -> Update params\n",
    "        ### Train\n",
    "        net.train()\n",
    "        loss = 0\n",
    "        count = 0\n",
    "\n",
    "        # Get data\n",
    "        data = torch.Tensor(X_train).to(device)\n",
    "        targets = torch.Tensor(y_train)\n",
    "        count += 1\n",
    "\n",
    "        # Transfer training data and targets to device\n",
    "        targets = Variable(targets).to(device)\n",
    "\n",
    "        # Send it through the model\n",
    "        output = net(data, n_train_samples).view(n_train_samples)\n",
    "        \n",
    "        # compute gradients given loss\n",
    "        loss = criterion(output, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        #optimizer.step()\n",
    "        \n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            output = net(data, n_train_samples).view(n_train_samples)\n",
    "            loss = criterion(output, targets)\n",
    "            loss.backward()\n",
    "            return loss\n",
    "        optimizer.step(closure)\n",
    "        \n",
    "        losses.append(loss)\n",
    "        \n",
    "        #eval\n",
    "        net.eval()\n",
    "        \n",
    "        ### Evaluate training\n",
    "        train_preds, train_targs = [], []\n",
    "\n",
    "        #for data, labels in training_generator:\n",
    "        train_data = torch.Tensor(X_train).to(device)\n",
    "        train_targets = torch.Tensor(y_train)\n",
    "        \n",
    "        train_output = net(train_data, n_train_samples).view(n_train_samples)\n",
    "        train_preds += list(train_output.data.cpu().numpy())\n",
    "        train_targs += list(train_targets)\n",
    "        \n",
    "        ### Evaluate validation\n",
    "        val_preds, val_targs = [], []\n",
    "\n",
    "        #for data, labels in validation_generator:\n",
    "        val_data = torch.Tensor(X_val).to(device)\n",
    "        val_targets = torch.Tensor(y_val)\n",
    "\n",
    "        val_output = net(val_data, n_val_samples).view(n_val_samples)\n",
    "        val_preds += list(val_output.data.cpu().numpy())\n",
    "        val_targs += list(val_targets)\n",
    "        \n",
    "        if np.isnan(train_output[0].detach().cpu().numpy()):\n",
    "            return best_model, train_err, valid_err, valid_err_cur_best#, min_val_output\n",
    "\n",
    "        train_err_cur = mean_squared_error(np.asarray(train_targs),np.asarray(train_preds))\n",
    "        valid_err_cur = mean_squared_error(np.asarray(val_targs),np.asarray(val_preds))\n",
    "        \n",
    "        if valid_err_cur < valid_err_cur_best:\n",
    "            best_model = copy.deepcopy(net)\n",
    "            valid_err_cur_best = valid_err_cur\n",
    "            min_val_output = val_output.data.cpu().numpy()\n",
    "\n",
    "        train_err[epoch] = train_err_cur\n",
    "        valid_err[epoch] = valid_err_cur\n",
    "        print(\"Epoch %2i : Train Err %f\" % (epoch+1, train_err_cur))\n",
    "        #if epoch % (np.floor(num_epochs/50)) == 0:\n",
    "            #print(\"Epoch %2i : Train Err %f\" % (epoch+1, train_err_cur))\n",
    "        #if epoch == num_epochs-1:\n",
    "            #print(\"Last epoch %2i : Final Train Err %f\" % (epoch+1, train_err_cur))\n",
    "        \n",
    "    return best_model, train_err, valid_err, valid_err_cur_best#, min_val_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rank = 5\n",
    "poly_order = 2\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "num_epochs = 500\n",
    "runs = 5\n",
    "\n",
    "train_errs = -1*np.ones((runs, num_epochs))\n",
    "\n",
    "for k in range(runs):\n",
    "    net = TTNet(n_features, poly_order, num_output, rank)\n",
    "    optimizer = optim.Adam(net.parameters(),lr=0.1)\n",
    "    #optimizer = optim.LBFGS(net.parameters(), max_iter=20, line_search_fn='strong_wolfe')\n",
    "    #optimizer = optim.SGD(net.parameters(), lr=0.2)\n",
    "    trained_model, train_err, valid_err = train_regression_model(net, optimizer, criterion, \n",
    "                                num_epochs, n_features, X_train, X_val, y_train, y_val)\n",
    "    \n",
    "    train_errs[k,0:len(train_err)] = train_err\n",
    "\n",
    "#savetxt('data/show_same_train_err_r5_n2_Adam.csv', train_errs, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation for poly_order and rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossvalidation_regression(models, K, X_train, X_val, y_train, y_val):\n",
    "    S = len(models)\n",
    "    \n",
    "    CV = model_selection.KFold(n_splits=K, shuffle=True, random_state=0)\n",
    "    \n",
    "    # observations to do CV on\n",
    "    X_CV = np.concatenate((X_train, X_val))\n",
    "    y_CV = np.concatenate((y_train, y_val))\n",
    "    \n",
    "    opt_models = []\n",
    "    min_errors = []\n",
    "    \n",
    "    valid_errors = -1*np.ones(K*S)\n",
    "    num_epochs = 100\n",
    "    \n",
    "    k=0\n",
    "    for par_index, test_index in CV.split(X_CV):\n",
    "        print('Computing CV fold: {0}/{1}..'.format(k+1,K))\n",
    "        \n",
    "        # extract training and test set for current CV fold\n",
    "        X_tr, y_tr = X_CV[par_index,:], y_CV[par_index]\n",
    "        X_va, y_va = X_CV[test_index,:], y_CV[test_index]\n",
    "        \n",
    "        min_error = 1e15\n",
    "        opt_model = None\n",
    "        \n",
    "        for s in range(S):\n",
    "            model = models[s]\n",
    "            \n",
    "            poly_order = model.poly_order\n",
    "            \n",
    "            # define optimizer and loss criterion\n",
    "            optimizer = optim.LBFGS(model.parameters())\n",
    "            criterion = nn.MSELoss()\n",
    "                \n",
    "            # train net\n",
    "            model_trained, train_err, valid_err, valid_err_cur_best = train_regression_model(\n",
    "                model, optimizer, criterion, num_epochs, n_features, X_tr, X_va, y_tr, y_va)\n",
    "            \n",
    "            valid_errors[k*S+s] = valid_err_cur_best\n",
    "            \n",
    "            error = valid_err_cur_best\n",
    "            if error < min_error:\n",
    "                min_error = error\n",
    "                opt_model = model_trained\n",
    "        opt_models.append(opt_model)\n",
    "        min_errors.append(min_error)\n",
    "        k+=1\n",
    "    return opt_models, min_errors, valid_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation TT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = range(3,15)\n",
    "poly_orders = range(1,4)\n",
    "models = []\n",
    "for rank in ranks:\n",
    "    for poly_order in poly_orders:\n",
    "        net = TTNet(n_features, poly_order, num_output, rank)\n",
    "        net.to(device)\n",
    "        models.append(net)\n",
    "\n",
    "K = 3\n",
    "opt_models, opt_models_min_errors, valid_errors = crossvalidation_regression(\n",
    "    models, K, X_train, X_val, y_train, y_val)\n",
    "\n",
    "#savetxt('data/cv_ranks.csv',ranks,delimiter=',')\n",
    "#savetxt('data/cv_poly_orders.csv',poly_orders,delimiter=',')\n",
    "#savetxt('data/cv_ttreg_valid_errors.csv',valid_errors,delimiter=',')\n",
    "\n",
    "opt_models_poly_order = []\n",
    "opt_models_rank = []\n",
    "for i in range(K):\n",
    "    opt_models_poly_order.append(opt_models[i].poly_order)\n",
    "    opt_models_rank.append(opt_models[i].rank)\n",
    "    print('optimal poly_order:', opt_models[i].poly_order)\n",
    "    print('optimal rank:', opt_models[i].rank)\n",
    "    print('minimal validation error of optimal model:', opt_models_min_errors[i])\n",
    "    \n",
    "#savetxt('data/cv_ttreg_opt_models_poly_order.csv', opt_models_poly_order,delimiter=',')\n",
    "#savetxt('data/cv_ttreg_opt_models_rank.csv', opt_models_rank,delimiter=',')\n",
    "#savetxt('data/cv_ttreg_opt_models_min_errors.csv', opt_models_min_errors,delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation TR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = loadtxt('data/cv_ranks.csv',delimiter=',')#range(3,15)\n",
    "poly_orders = loadtxt('data/cv_poly_orders.csv',delimiter=',')#range(1,4)\n",
    "print(ranks)\n",
    "print(poly_orders)\n",
    "models = []\n",
    "for rank in ranks:\n",
    "    for poly_order in poly_orders:\n",
    "        net = TRNet(int(n_features), int(poly_order), int(num_output), int(rank))\n",
    "        net.to(device)\n",
    "        models.append(net)\n",
    "\n",
    "K = 3\n",
    "opt_models, opt_models_min_errors, valid_errors = crossvalidation_regression(\n",
    "    models, K, X_train, X_val, y_train, y_val)\n",
    "\n",
    "#savetxt('data/cv_ranks.csv',ranks,delimiter=',')\n",
    "#savetxt('data/cv_poly_orders.csv',poly_orders,delimiter=',')\n",
    "#savetxt('data/cv_trreg_valid_errors.csv',valid_errors,delimiter=',')\n",
    "\n",
    "opt_models_poly_order = []\n",
    "opt_models_rank = []\n",
    "for i in range(K):\n",
    "    opt_models_poly_order.append(opt_models[i].poly_order)\n",
    "    opt_models_rank.append(opt_models[i].rank)\n",
    "    print('optimal poly_order:', opt_models[i].poly_order)\n",
    "    print('optimal rank:', opt_models[i].rank)\n",
    "    print('minimal validation error of optimal model:', opt_models_min_errors[i])\n",
    "    \n",
    "#savetxt('data/cv_trreg_opt_models_poly_order.csv', opt_models_poly_order,delimiter=',')\n",
    "#savetxt('data/cv_trreg_opt_models_rank.csv', opt_models_rank,delimiter=',')\n",
    "#savetxt('data/cv_trreg_opt_models_min_errors.csv', opt_models_min_errors,delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation CP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = loadtxt('data/cv_ranks.csv',delimiter=',')#range(3,15)\n",
    "poly_orders = loadtxt('data/cv_poly_orders.csv',delimiter=',')#range(1,4)\n",
    "print(ranks)\n",
    "print(poly_orders)\n",
    "models = []\n",
    "for rank in ranks:\n",
    "    for poly_order in poly_orders:\n",
    "        net = CPNet(int(n_features), int(poly_order), int(num_output), int(rank))\n",
    "        net.to(device)\n",
    "        models.append(net)\n",
    "\n",
    "K = 3\n",
    "opt_models, opt_models_min_errors, valid_errors = crossvalidation_regression(\n",
    "    models, K, X_train, X_val, y_train, y_val)\n",
    "\n",
    "#savetxt('data/cv_ranks.csv',ranks,delimiter=',')\n",
    "#savetxt('data/cv_poly_orders.csv',poly_orders,delimiter=',')\n",
    "#savetxt('data/cv_cpreg_valid_errors.csv',valid_errors,delimiter=',')\n",
    "\n",
    "opt_models_poly_order = []\n",
    "opt_models_rank = []\n",
    "for i in range(K):\n",
    "    opt_models_poly_order.append(opt_models[i].poly_order)\n",
    "    opt_models_rank.append(opt_models[i].rank)\n",
    "    print('optimal poly_order:', opt_models[i].poly_order)\n",
    "    print('optimal rank:', opt_models[i].rank)\n",
    "    print('minimal validation error of optimal model:', opt_models_min_errors[i])\n",
    "    \n",
    "#savetxt('data/cv_cpreg_opt_models_poly_order.csv', opt_models_poly_order,delimiter=',')\n",
    "#savetxt('data/cv_cpreg_opt_models_rank.csv', opt_models_rank,delimiter=',')\n",
    "#savetxt('data/cv_cpreg_opt_models_min_errors.csv', opt_models_min_errors,delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rank vs weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find optimal weight decays for each rank\n",
    "num_epochs = 100\n",
    "poly_order = 2\n",
    "ranks = np.arange(1,11) #[1,...,10]\n",
    "weights = np.asarray(\n",
    "    [0, 1.e-10, 1.e-9, 1.e-8, 1.e-7, 1.e-6, 1.e-5, 1.e-4, 1.e-3, 1.e-2, 1.e-1, 1.e+0])\n",
    "\n",
    "# run model with adam optim for each weight and each rank. see which weights gives the best validation error\n",
    "\n",
    "models = []\n",
    "for rank in ranks:\n",
    "    model = TTNet(n_features, poly_order, num_output, rank)\n",
    "    model.to(device)\n",
    "    models.append(model)\n",
    "\n",
    "train_err_models = np.zeros((len(models)*len(weights), num_epochs))\n",
    "valid_err_models = np.zeros((len(models)*len(weights), num_epochs))\n",
    "\n",
    "counter = 0\n",
    "for i in range(len(models)):\n",
    "    model = models[i]\n",
    "    for j in range(len(weights)):\n",
    "        print(\"Model:\",i+1,j)\n",
    "        weight = weights[j]\n",
    "        #optimizer = optim.LBFGS(model.parameters(), line_search_fn='strong_wolfe')\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.1, weight_decay=weight)\n",
    "        criterion = nn.MSELoss()\n",
    "        trained_model, train_err, valid_err, valid_err_cur_best = train_regression_model(\n",
    "            model, optimizer, criterion, num_epochs, n_features, X_train, X_val, y_train, y_val)\n",
    "        train_err_models[counter] = train_err\n",
    "        valid_err_models[counter] = valid_err\n",
    "        counter += 1\n",
    "\n",
    "# save to csv file\n",
    "\n",
    "#savetxt('data/weight/weights.csv', weights, delimiter=',')\n",
    "#savetxt('data/weight/ranks.csv', ranks, delimiter=',')\n",
    "#savetxt('data/weight/train_err_models.csv', train_err_models, delimiter=',')\n",
    "#savetxt('data/weight/valid_err_models.csv', valid_err_models, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run optimal models and compare with test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''% TT Reg\n",
    "% poly order 2\n",
    "% rank 5\n",
    "% err 0.3025366067886352539\n",
    "\n",
    "% TR Reg\n",
    "% poly order 2\n",
    "% rank 9\n",
    "% err 0.3025366067886352539\n",
    "\n",
    "% CP Reg\n",
    "% poly order 2\n",
    "% rank 14\n",
    "% err 0.3213250637054443359'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_test(net, X_test, y_test, n_features):\n",
    "    \n",
    "    n_test_samples = len(X_test[:,0])\n",
    "    X_test = vandermonde_vec(X_test, n_test_samples, n_features, net.poly_order)\n",
    "    # reshape X_train, X_val to 3d vandermonde for tensor ring and cpd\n",
    "    if (net.type == 'tr') or (net.type == 'cp'):\n",
    "        X_test = np.reshape(X_test, (n_test_samples, n_features, net.poly_order))\n",
    "    \n",
    "    data = torch.Tensor(X_test)\n",
    "    targets = torch.Tensor(y_test)\n",
    "    targets = Variable(targets.long())\n",
    "    output = net(data, n_test_samples).view(n_test_samples)\n",
    "\n",
    "    test_err = mean_squared_error(np.asarray(output.data.cpu().numpy()), np.asarray(targets))\n",
    "\n",
    "    print(\"\\nTest set MSerror:\",test_err)\n",
    "    #for name, param in net.named_parameters():\n",
    "        #if param.requires_grad:\n",
    "            #print(name)\n",
    "            #print(param.data.size())\n",
    "            #print(param.data)\n",
    "    return test_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TT\n",
    "num_epochs = 100\n",
    "poly_order = 2\n",
    "rank = 5\n",
    "num_output = 1\n",
    "\n",
    "net = TTNet(int(n_features), int(poly_order), int(num_output), int(rank))\n",
    "optimizer = optim.LBFGS(net.parameters(), line_search_fn='strong_wolfe')\n",
    "criterion = nn.MSELoss()\n",
    "#net.to(device)\n",
    "model_trained, train_err, valid_err, valid_err_cur_best, min_val_output = train_regression_model(\n",
    "            net, optimizer, criterion, num_epochs, n_features, X_train, X_val, y_train, y_val)\n",
    "\n",
    "savetxt('data/min_val_output_tt.csv', min_val_output, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TR\n",
    "num_epochs = 20\n",
    "poly_order = 2\n",
    "rank = 9\n",
    "num_output = 1\n",
    "\n",
    "net = TRNet(n_features, poly_order, num_output, rank)\n",
    "optimizer = optim.LBFGS(net.parameters(), line_search_fn='strong_wolfe',max_iter=20)\n",
    "#optimizer = optim.Adam(net.parameters(), lr=0.1)\n",
    "criterion = nn.MSELoss()\n",
    "#net.to(device)\n",
    "model_trained, train_err, valid_err, valid_err_cur_best, min_val_output = train_regression_model(\n",
    "            net, optimizer, criterion, num_epochs, n_features, X_train, X_val, y_train, y_val)\n",
    "\n",
    "savetxt('data/min_val_output_tr.csv', min_val_output, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CP\n",
    "num_epochs = 100\n",
    "poly_order = 2\n",
    "rank = 14\n",
    "num_output = 1\n",
    "\n",
    "net = CPNet(int(n_features), int(poly_order), int(num_output), int(rank))\n",
    "optimizer = optim.LBFGS(net.parameters(), line_search_fn='strong_wolfe')\n",
    "criterion = nn.MSELoss()\n",
    "#net.to(device)\n",
    "model_trained, train_err, valid_err, valid_err_cur_best, min_val_output = train_regression_model(\n",
    "            net, optimizer, criterion, num_epochs, n_features, X_train, X_val, y_train, y_val)\n",
    "\n",
    "savetxt('data/min_val_output_cp.csv', min_val_output, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
